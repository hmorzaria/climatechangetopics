---
title: "Climate Change Framing for Adaptation in the Mexican Pacific"
output:
  html_document:
    df_print: paged
---

####Hem Nalini Morzaria-Luna. CEDO Intercultural
####Gabriela Cruz-Piñón. Departamento de Ciencias Marinas y Costeras. Universidad Autónoma de Baja California Sur.
####Andrea K. Gerlak. School of Geography and Development. Associate Research Professor
####Luis E. Calderón-Aguilera. Departamento de Ecología Marina. Centro de Investigación Científica y de Educación Superior de Ensenada
####Peggy Turk-Boyer. CEDO Intercultural

Climate change impacts on coastal communities in Northwest Mexico require the implementation of adaptation strategies. Artisanal fisheries play a key social and economic role in these communities. Fisher understanding and acceptance of climate change information and perceptions of environmental change will shape and influence policy implementation; fisher’s experiences can also help inform new policy adoption. 
We assess how national, regional and local news media interprets and communicates climate change messages in the region bordering the Gulf of California, Mexico. Here, printed and online news are an important source of information, and the climate change frames they present reach artisanal fishers in the region. This project is aimed at increasing scientific capacity to respond to climate change impacts in the Northern Gulf of California and serves as a proof-of-concept for subsequent analyses in other coastal areas of the Mexican Pacific.

Our results can be used to aid in the implementation of adaptation strategies to climate change impacts and inform resource management efforts that consider climate change within their goals. 


```{r sourcefiles, message=FALSE, warning=FALSE, include=FALSE}

source("libraries.R")
source("Sentiment_analysis_functions.R")
source("cognitive_services_functions.R")

```

We developed a database of news articles published by local and regional newspapers in the states that border the Gulf of California, Sonora, Sinaloa, Nayarit, Jalisco, Baja California, and Baja California Sur between 01/01/2016 and 12/31/2016 archived online. 
We found 2,645 articles that were used to pupulate the database.

```{r get texts, message=FALSE, warning=FALSE, include=FALSE}

html.list <- fread("list_links.csv", sep=",", header=TRUE, encoding = "Latin-1") %>% tbl_df()

text.list <- html.list %>% 
  filter(!is.na(ARTICULO)) %>% 
  dplyr::select(ARTICULO) %>% 
  rename(text=ARTICULO)

link.list <- html.list %>% 
  filter(is.na(ARTICULO)) %>% 
  pull(LINK_NOTA)

source.list <- fread("list_sources.csv", sep=",", header=TRUE) %>% tbl_df()


#use this to scrape articles for 2016 from websites
#html.texts <- lapply(link.list, get_articles)

#html.texts %>% 
#  bind_rows %>% 
#  write_csv("html_text.csv")

```


```{r load_data, echo=FALSE, message=FALSE, warning=FALSE}

#this is the result of scraping data
article.texts <- clean_html(file="html_text.csv", random = "random_words.csv",additional = text.list)
 

print(head(article.texts))

  
```

We first ‘tokenized’ each article, extracting all words per article. We found 38,106 unique words. We then filtered out the stopwords, which are commonly used words (such as “the”), that are not usually considered in natural language processing. We derived the list of stopwords in Spanish from two existing lists that we combined, extracted unique words, and checked for accuracy before filtering the database. The list of words without stopwords had 579,121 tokens. 

```{r get_words, message=FALSE, warning=FALSE, include=FALSE}

#stopword final list, reviewed manually
stop.words <- fread("stopwords_spanish.csv", header= TRUE, encoding = "UTF-8") %>% 
 pull(word)


article.num <- 1:nrow(article.texts)

word.list <- lapply(article.num, get_words, article.texts,random="random_words.csv")

word.tbl <- word.list %>% 
  bind_rows %>% 
  anti_join(stop.words, by = "word")

```


This word cloud shows the word frequency.

```{r word_floud_freq, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

word.cloud <- get_wordcloud(word.tbl)

```

###Sentiment Analysis

Initially, we carried out a sentiment analysis, which determines the attitude of the writer by categorizing text as positive or negative (Barkemeyer et al. 2015). We classified words as negative or positive based on five existing lexicons. The combined sentiment lexicon was revised for duplicate terms, conflicting sentiments (words assigned to both categories), and accuracy before applying. The resulting sentiment lexicon had 14,862 terms.
We could assign sentiment to 180,255 tokens.

```{r getwords, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

#this all the combined lexicons and was reviewed manually   

sent.lexicon <- read_csv("spanish_lexicon.csv") %>% 
  mutate(word=tolower(word)) %>% 
  distinct(word,category) %>% 
  arrange(category,word) 

sentiment.tokens <- get_sentiment(word.tbl,sent.lexicon)

```

```{r sentiment_analysis, echo=FALSE, message=FALSE, warning=FALSE}
#graphs

sentiment.barplot <- get_sentiment_barplot(sentiment.tokens)
sentiment.barplot
```

This is the result of the sentiment analysis, showing the number of positive vs negative words per article.

```{r word_cloud_comparison, echo=FALSE, message=FALSE, warning=FALSE}

sentiment.cloud <- get_sentiment_wordcloud(sentiment.tokens)

sentiment.cloud
```


###Topic Modelling

We applied topic modeling, which identifies ‘topics’ or ‘themes’ in the body of the text. We use unsupervised machine learning, where we provide no input on how the articles should be classified or apply set pre-interpretations to identify word clusters that can be interpreted as frames, but rather rely on word co-occurrence. We then validate the models internally by examining different combinations of topics and selecting those that link climate change to a coherent set of other concepts. Finally, we will validate the models externally by reviewing that they represent frames that are consistent and/or have been previously identified in the climate change literature. Once the test models are validated, we will use the model to classify a sample of articles published between 2018-2019, and develop the corresponding models of topic frames. 


```{r topic_model, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

model.lda.results <- get_topic(article.texts,no_topics=15,stop.words)

  summary(model.lda.results$model$coherence)
   
  model.lda.results$model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ][ 1:10 , ]
  

saveRDS(model.lda.results$model, "./lda_model.rds")


```

```{r}

topic.list.d

topic.predict <- predict(topic.list.d,term.matrix)

```

